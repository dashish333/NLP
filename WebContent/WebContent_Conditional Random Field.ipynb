{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional Random Fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In NLP, it is a common task to extract words/ phrases of particular types from a given sentence or paragraph. For example, when performing analysis of a corpus of travel articles, we may want to know which travelling destination are mentioned in the articles, and how many articles are related to each of these travelling document.\n",
    "##### Conditional Random Fields are discriminative model used for predicting the sequence of labels.<br><br> It uses information from the context through previous labels and thus helping the model in turn to make better prediction for unseen text/word etc.This technqiue can be very well used for Name Entity Recognition(NER), which is to give a accurate label to given word(incase of NLP).<br><br>For Example:<br> $\\bullet\\textit{\"Shahjahan went to see Taj Mahal\"}$ - in this example the $\\textit{Taj Mahal}$ can be location representing a monument in Agra or can correspond to Tea Bags.<br>$\\bullet\\textit{\"This Apple product is one in its segment\"}$ - now here should the word Apple is to be associated with the Apple(Tech company) or a food item.<br> Hence, given a sequence of words identifying the correct sequence of labels is the problem to be solved and depending upon the problem finding the correct sequence of label, where label can be ${person, location, organization, etc}$ for instance. So, the CRF can solve these kind of problem which is a challenging task for the old traditional graphical models like $\\underline{HMM}$ or $\\underline{MaxEnt}$.<br><br>Let's try to understand through a bit more theory first and then I'll give a small implementation of CRF.I'll highlight few of the keywords discussed above first.\n",
    "\n",
    "### Discriminative Model: \n",
    "##### In Machine Learning we have two different types of modelling technqiue.<br> $\\bullet$ Discriminative - Logistic Regression, classifier based on Maximum Likelihood Estimation<br>$\\bullet$ Generative - Naive Bayes is popular and simple probablistic classifier<br>\n",
    "\n",
    "### CRF Model: A special case of undirected graphical model.\n",
    "##### So, the basic crux of the CRF is to generate labels given huge amount of data where the input data is sequential. Also we consider previous context while making a prediction for given data point. <br><br>$\\bullet$Let ${W}$ be the tokens in the document and ${w_{i}}$ be the corresponding word observed. <br>$\\bullet$ ${y_i}$ be the hidden label.<br> The conditional distribtuion is modeled as: <br>\\begin{equation*} \\hat{y} = \\underset{y}{\\operatorname{argmax}} P(y|x)\\end{equation*} <br><br> To model the appropriate behaviour of CRF we need a {feature function} which have the below parameters:<br> $\\bullet$ Set of Input Vectors - W <br>$\\bullet$ i - the word for which we want to predict label<br> $\\bullet$ Label for data points ${w_{1:i-1}}$<br> $\\bullet$ label of point $i$ in W <br> The feature functions are the key components of CRF and in our special case of linear-chain CRF, and the general form is: \\begin{equation*}f_1(y_{i-1},y_{i},w_{1:N},i)\\end{equation*} which looks at a pair of adjacent labels $y_{i−1}$, $y_i$, the whole input sequence $w_{1:N}$ , and the current position $i$. <br><br> For Ex: we can define a simple feature function which produces binary values: $\\textit{1}$ for the current word is ${Taj Mahal}$, and if the current state ${y_n}$ is ${Monument}$.<br>Usage of such feature depends on the corresponding weight ${\\lambda_1}$. If the ${\\lambda_1 \\gt 0}$ then label ${Monument}$ is preferred for above example otherwise CRF will try to avoid the label ${Monument}$ for the text ${TajMahal}$\n",
    "\n",
    "### CRF vs HMM:\n",
    "##### Although both are used to model sequential data, they are different algorithms.<br>Hidden Markov Models are generative, and give output by modeling the joint probability distribution whereeas Conditional Random Fields as mentioned above are discriminative, and model the conditional probability distribution. CRFs don’t rely on the independence assumption (labels are independent of each other), and avoid label bias. One way to look at it is that Hidden Markov Models are a very specific case of Conditional Random Fields, with constant transition probabilities used instead. HMMs are based on Naive Bayes, which we say can be derived from Logistic Regression, from which CRFs are derived.\n",
    "\n",
    "### CRF Application:\n",
    "##### $\\bullet$ Part-of-Speech Tagging (POS).<br> $\\bullet$ Named Entity Recognition.<br>$\\bullet$ Image segmentation \n",
    "\n",
    "### Disadvantage:\n",
    "##### Highly computational cost as well as complexity at the training stage of the algorithm. Re-training on new data is difficult."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minor Implementation:\n",
    "#### Sequence labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from bs4.element import Tag\n",
    "import codecs\n",
    "import pycrfsuite\n",
    "#nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(doc, i):\n",
    "    #print(doc,i)\n",
    "    word = doc[i][0]\n",
    "    postag = doc[i][1]\n",
    "    # Common features for all words\n",
    "    features = [\n",
    "        'bias',\n",
    "        'word.lower=' + word.lower(),\n",
    "        'word[-2:]=' + word[-2:],\n",
    "        'word.isupper=%s' % word.isupper(),\n",
    "        'word.isdigit=%s' % word.isdigit(),\n",
    "        'postag=' + postag\n",
    "                ]\n",
    "    #print(features)\n",
    "    # Features for words that are not at the beginning of a document\n",
    "    if i > 0:\n",
    "        word1 = doc[i-1][0]\n",
    "        postag1 = doc[i-1][1]\n",
    "        features.extend([\n",
    "            '-1:word.lower=' + word1.lower(),\n",
    "            '-1:word.isupper=%s' % word1.isupper(),\n",
    "            '-1:word.isdigit=%s' % word1.isdigit(),\n",
    "            '-1:postag=' + postag1\n",
    "        ])\n",
    "    else:\n",
    "        # Indicate that it is the 'beginning of a document'\n",
    "        features.append('<s>')\n",
    "\n",
    "    # Features for words that are not at the end of a document\n",
    "    if i < len(doc)-1:\n",
    "        word1 = doc[i+1][0]\n",
    "        postag1 = doc[i+1][1]\n",
    "        features.extend([\n",
    "            '+1:word.lower=' + word1.lower(),\n",
    "            '+1:word.isupper=%s' % word1.isupper(),\n",
    "            '+1:word.isdigit=%s' % word1.isdigit(),\n",
    "            '+1:postag=' + postag1\n",
    "        ])\n",
    "    else:\n",
    "        # Indicate that it is the 'end of a document'\n",
    "        features.append('</s>')\n",
    "    #print(features,\"\\n\")\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting features in documents\n",
    "def extract_features(doc):\n",
    "    return [word2features(doc, i) for i in range(len(doc))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating the list of labels for each document\n",
    "def get_labels(doc):\n",
    "    return [label for (token, postag, label) in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature generation\n",
      "type: CRF1d\n",
      "feature.minfreq: 0.000000\n",
      "feature.possible_states: 0\n",
      "feature.possible_transitions: 1\n",
      "0....1....2....3....4....5....6....7....8....9....10\n",
      "Number of features: 104\n",
      "Seconds required: 0.004\n",
      "\n",
      "L-BFGS optimization\n",
      "c1: 0.200000\n",
      "c2: 0.010000\n",
      "num_memories: 6\n",
      "max_iterations: 20\n",
      "epsilon: 0.000010\n",
      "stop: 10\n",
      "delta: 0.000010\n",
      "linesearch: MoreThuente\n",
      "linesearch.max_iterations: 20\n",
      "\n",
      "***** Iteration #1 *****\n",
      "Loss: 79.321718\n",
      "Feature norm: 1.000000\n",
      "Error norm: 60.827283\n",
      "Active features: 104\n",
      "Line search trials: 1\n",
      "Line search step: 0.006045\n",
      "Seconds required for this iteration: 0.000\n",
      "\n",
      "***** Iteration #2 *****\n",
      "Loss: 52.427299\n",
      "Feature norm: 1.611210\n",
      "Error norm: 63.767277\n",
      "Active features: 101\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.000\n",
      "\n",
      "***** Iteration #3 *****\n",
      "Loss: 30.925108\n",
      "Feature norm: 2.883504\n",
      "Error norm: 52.350005\n",
      "Active features: 102\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.000\n",
      "\n",
      "***** Iteration #4 *****\n",
      "Loss: 22.161603\n",
      "Feature norm: 3.172498\n",
      "Error norm: 16.144747\n",
      "Active features: 102\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.000\n",
      "\n",
      "***** Iteration #5 *****\n",
      "Loss: 19.502325\n",
      "Feature norm: 3.512747\n",
      "Error norm: 10.496563\n",
      "Active features: 104\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.000\n",
      "\n",
      "***** Iteration #6 *****\n",
      "Loss: 17.244077\n",
      "Feature norm: 3.882162\n",
      "Error norm: 8.464430\n",
      "Active features: 104\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.001\n",
      "\n",
      "***** Iteration #7 *****\n",
      "Loss: 14.287722\n",
      "Feature norm: 4.689839\n",
      "Error norm: 5.401823\n",
      "Active features: 102\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.000\n",
      "\n",
      "***** Iteration #8 *****\n",
      "Loss: 12.762613\n",
      "Feature norm: 5.094290\n",
      "Error norm: 6.317320\n",
      "Active features: 89\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.000\n",
      "\n",
      "***** Iteration #9 *****\n",
      "Loss: 11.695173\n",
      "Feature norm: 5.569973\n",
      "Error norm: 1.702143\n",
      "Active features: 89\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.000\n",
      "\n",
      "***** Iteration #10 *****\n",
      "Loss: 11.082592\n",
      "Feature norm: 6.083873\n",
      "Error norm: 2.562573\n",
      "Active features: 82\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.000\n",
      "\n",
      "***** Iteration #11 *****\n",
      "Loss: 10.561426\n",
      "Feature norm: 6.274897\n",
      "Error norm: 1.203900\n",
      "Active features: 82\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.000\n",
      "\n",
      "***** Iteration #12 *****\n",
      "Loss: 10.086658\n",
      "Feature norm: 6.715671\n",
      "Error norm: 1.139447\n",
      "Active features: 81\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.000\n",
      "\n",
      "***** Iteration #13 *****\n",
      "Loss: 9.827553\n",
      "Feature norm: 7.031017\n",
      "Error norm: 2.511805\n",
      "Active features: 72\n",
      "Line search trials: 2\n",
      "Line search step: 0.500000\n",
      "Seconds required for this iteration: 0.000\n",
      "\n",
      "***** Iteration #14 *****\n",
      "Loss: 9.667957\n",
      "Feature norm: 7.101143\n",
      "Error norm: 0.861164\n",
      "Active features: 69\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.000\n",
      "\n",
      "***** Iteration #15 *****\n",
      "Loss: 9.518807\n",
      "Feature norm: 7.074786\n",
      "Error norm: 0.977852\n",
      "Active features: 67\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.000\n",
      "\n",
      "***** Iteration #16 *****\n",
      "Loss: 9.416810\n",
      "Feature norm: 7.402779\n",
      "Error norm: 3.133817\n",
      "Active features: 53\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.000\n",
      "\n",
      "***** Iteration #17 *****\n",
      "Loss: 9.256899\n",
      "Feature norm: 7.317603\n",
      "Error norm: 0.939363\n",
      "Active features: 56\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.000\n",
      "\n",
      "***** Iteration #18 *****\n",
      "Loss: 9.195889\n",
      "Feature norm: 7.423976\n",
      "Error norm: 1.693188\n",
      "Active features: 55\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.000\n",
      "\n",
      "***** Iteration #19 *****\n",
      "Loss: 9.122030\n",
      "Feature norm: 7.461569\n",
      "Error norm: 0.623161\n",
      "Active features: 50\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.000\n",
      "\n",
      "***** Iteration #20 *****\n",
      "Loss: 9.087088\n",
      "Feature norm: 7.607959\n",
      "Error norm: 1.489691\n",
      "Active features: 49\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.000\n",
      "\n",
      "L-BFGS terminated with the maximum number of iterations\n",
      "Total seconds required for training: 0.008\n",
      "\n",
      "Storing the model\n",
      "Number of active features: 49 (104)\n",
      "Number of active attributes: 31 (70)\n",
      "Number of active labels: 2 (2)\n",
      "Writing labels\n",
      "Writing attributes\n",
      "Writing feature references for transitions\n",
      "Writing feature references for attributes\n",
      "Seconds required: 0.001\n",
      "\n",
      "taj (N)\n",
      "mahal (N)\n",
      "is (I)\n",
      "in (I)\n",
      "agra (N)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          I       1.00      1.00      1.00        27\n",
      "          N       1.00      1.00      1.00        33\n",
      "\n",
      "avg / total       1.00      1.00      1.00        60\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read data file and parse the XML\n",
    "with codecs.open(\"testFile.xml\", \"r\", \"utf-8\") as infile:\n",
    "    soup = bs(infile, \"html5lib\")\n",
    "#print(soup.prettify(),\"\\n\")\n",
    "for elem in soup.find_all(\"document\"):\n",
    "    textContent = []\n",
    "    # Loop through each child of the element under \"textwithnamedentities\"\n",
    "    for c in elem.find(\"textwithnamedentities\").children:\n",
    "        if type(c) == Tag:\n",
    "            if c.name == \"namedentityintext\":\n",
    "                label = \"N\"  # part of a named entity\n",
    "            else:\n",
    "                label = \"I\"  # irrelevant word\n",
    "            for w in c.text.split(\" \"):\n",
    "                if len(w) > 0:\n",
    "                    textContent.append((w, label))\n",
    "    docs.append(textContent)\n",
    "data = []\n",
    "for i, doc in enumerate(docs):\n",
    "    # fetching list of tokens in the document\n",
    "    for _tuple in doc:\n",
    "        tokens=_tuple[0]\n",
    "    #POS tagging\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    # creating a list of word-pos tag- label(N/I)\n",
    "    data.append([(w, pos,label) for (w, label), (word, pos) in zip(doc, tagged)])\n",
    "X = [extract_features(doc) for doc in data]\n",
    "Y = [get_labels(doc) for doc in data]\n",
    "#splitting data for training the data\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n",
    "\n",
    "trainer = pycrfsuite.Trainer(verbose=True)\n",
    "\n",
    "# Submit training data to the trainer\n",
    "for xseq, yseq in zip(X_train, Y_train):\n",
    "    trainer.append(xseq, yseq)\n",
    "\n",
    "# Set the parameters of the model\n",
    "trainer.set_params({\n",
    "    # coefficient for L1 penalty\n",
    "    'c1': 0.2,\n",
    "    # coefficient for L2 penalty\n",
    "    'c2': 0.01,  \n",
    "    # maximum number of iterations\n",
    "    'max_iterations': 20,\n",
    "    # whether to include transitions that\n",
    "    # are possible, but not observed\n",
    "    'feature.possible_transitions': True\n",
    "})\n",
    "\n",
    "# Provide a file name as a parameter to the train function, to save the model when training is finished\n",
    "trainer.train('crf.model')\n",
    "#print(X_test)\n",
    "# Generate predictions\n",
    "tagger = pycrfsuite.Tagger()\n",
    "tagger.open('crf.model')\n",
    "Y_pred = [tagger.tag(xseq) for xseq in X_test]\n",
    "\n",
    "# Let's take a look at a random sample in the testing set\n",
    "i = 2\n",
    "for x, y in zip(Y_pred[i], [x[1].split(\"=\")[1] for x in X_test[i]]):\n",
    "    print(\"%s (%s)\" % (y, x))\n",
    "\n",
    "# Create a mapping of labels to indices\n",
    "labels = {\"N\": 1, \"I\": 0}\n",
    "\n",
    "# Convert the sequences of tags into a 1-dimensional array\n",
    "predictions = np.array([labels[tag] for row in y_pred for tag in row])\n",
    "truths = np.array([labels[tag] for row in y_test for tag in row])\n",
    "\n",
    "# Print out the classification report\n",
    "print(classification_report(truths, predictions,target_names=[\"I\", \"N\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PY35",
   "language": "python",
   "name": "ml-lib"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
